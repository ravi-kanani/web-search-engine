skip to main content css-tricks articles videos almanac snippets newsletter jobs guides back to top contact about archives advertise jobs license subscribe forums guest posting facebook twitter youtube instagram rss search for: search open search open navigation home / articles / consistent backends and ux: what can go wrong? author brecht de rooms comments start conversation last updated mar 26, 2020 personalized marketing with mailchimp article series why should you care? what can go wrong? what are the barriers to adoption? how do new algorithms help? in the previous article, we explained what strong (vs. eventual) consistency is. this article is the second part of a series where we explain how a lack of strong consistency makes it harder to deliver a good end-user experience, can bring serious engineering overhead, and opens you up to exploits. this part is longer since we will explain different database anomalies, go through several example scenarios, and briefly highlight which kind of database suffers from each anomaly. user experience is the driving factor in the success of any app, and relying on an inconsistent backend can increase the challenge to deliver a good experience. more importantly, building application logic on top of inconsistent data can lead to exploits. one paper calls these kinds of attacks "acidrain." they investigated 12 of the most popular self-hosted e-commerce applications and at least 22 possible critical attacks were identified. one website was a bitcoin wallet service that had to shut down due to these attacks. when you choose a distributed database that is not 100% acid, there will be dragons. as explained in one of our previous examples, due to misinterpretations, badly defined terminology, and aggressive marketing, it is very hard for an engineer to determine what guarantees a specific database delivers. which dragons? your app might feature issues such as wrong account balances, unreceived user rewards, trade transactions that executed twice, messages that appear out of order, or application rules that are violated. for a quick introduction why distributed databases are necessary and difficult, please refer to our first article or this excellent video explanation. in short, a distributed database is a database that holds copies of your data in multiple locations for scale, latency, and availability reasons we’ll go through four of these potential issues (there are more) and illustrate them with examples from game development. game development is complex and those developers are faced with many problems that closely resemble serious real-life problems. a game has trading systems, messaging systems, awards that require conditions to be fulfilled, etc. remember how angry (or happy 🤨) gamers can be if things go wrong or appear to go wrong. in games, user experience is everything, so game developers are often under huge pressure to make sure their systems are fault-tolerant. ready? let’s dive into the first potential issue! 1. stale reads stale reads are reads that return old data, or in other words, data that returns values which are not yet updated according to the latest writes. many distributed databases, including traditional databases that scale up with replicas (read part 1 to learn how these work), suffer from stale reads. impact on end users first off, stale reads can affect end users. and it's not a single impact. frustrating experiences and unfair advantages imagine a scenario where two users in a game encounter a chest with gold. the first user receives the data from one database server while the second is connected to a second database server. the order of events goes as follows: user 1 (via database server 1) sees and opens the chest, retrieves the gold. user 2 (via database server 2) sees a full chest, opens it, and fails. user 2 still sees a full chest and does not understand why it fails. although this seems like a minor problem, the result is a frustrating experience for the second player. not only did he have a disadvantage, but he will also often see situations in the game where things appear to be there, yet they are not. next, let’s look at an example where the player takes action on a stale read! stale reads leading to duplicated writes imagine a situation where a character in the game tries to buy a shield and a sword in a shop. if there are multiple locations that contain the data and there is no intelligent system in place to provide consistency, then one node will contain older data than another. in that case, the user might buy the items (which contacts the first node) and then check his inventory (which contacts the second node), only to see that they are not there. the user will probably be confused and might think that the transaction didn’t go through. what would most people do in that case? well, they try to buy the item again. once the second node has caught up, the user has already bought a duplicate, and once the replica catches up, he suddenly sees that he has no money left and two items of each. he is left with the perception that our game is broken. example of a user requesting the same transaction twice due to eventual consistency (t1) - a player buys a shield and sword. this buy transaction is committed to the master node. (r1) - the player loads his inventory, but the read hits replica1. since (t1) is not yet replicated, he does not see his items. (rt1) - the first transaction is replicated, yet too late to have an effect on (r1) (t2) - the player thinks his buy attempt failed and buys the sword and shield again. (rt2) - the second transaction is replicated. (r2) - the player loads his inventory, and now sees he has two shields, two swords, and almost no gold left. in this case, the user has spent resources which he did not want to spend. if we write an email client on top of such a database, a user might try to send an email, then refresh the browser and not be able to retrieve the email he has just sent, and therefore send it again. delivering a good user experience and implementing secure transactions such as bank transactions on top of such a system is notoriously hard. impact on developers when coding, you always have to expect that something is not there (yet) and code accordingly. when reads are eventually consistent, writing fault-proof code becomes very challenging and chances are that users will encounter problems in your application. when reads are eventually consistent, these problems will be gone by the time you are able to investigate them. basically, you end up chasing ghosts. developers still often choose databases or distribution approaches that are eventually consistent since it often takes time to notice the problems. then, once the problems in their application arise, they try to be creative and build solutions (1, 2) on top of their traditional database to fix the stale reads. the fact that there are many guides like this and that databases like cassandra have implemented some consistency features shows that these problems are real and do cause issues in production systems more frequently than you might imagine. custom solutions on top of a system that is not built for consistency are very complex and brittle. why would someone go through such a hassle if there are databases that deliver strong consistency out-of-the-box? databases that exhibit this anomaly traditional databases (postgresql, mysql, sql server, etc..) that use master-read replication typically suffer from stale reads. many newer distributed databases also started off as eventually consistent, or in other words, without protection against stale reads. this was due to a strong belief in the developer community that this was necessary to scale. the most famous database that started off like this is cassandra, but cassandra recognized how their users struggled to deal with this anomaly and have since provided extra measures to avoid this. older databases or databases which are not designed to provide strong consistency in an efficient way such as cassandra, couchdb, and dynamodb are by default eventually consistent. other approaches such as riak are also eventually consistent, but take a different path by implementing a conflict resolution system to reduce the odds of outdated values. however, this does not guarantee that your data is safe since conflict resolution is not fault-proof. 2. lost writes in the realm of distributed databases, there is an important choice to make when writes happen at the same time. one option (the safe one) is to make sure that all database nodes can agree on the order of these writes. this is far from trivial since it either requires synchronized clocks, for which specific hardware is necessary, or an intelligent algorithm like calvin that doesn't rely on clocks. the second, less safe option is to allow each node to write locally and then decide what to do with the conflicts later on. databases that choose the second option can lose your writes. two database choices, avoid conflicts by ordering transactions or allow conflicts and resolve them. impact on end users consider two trade transactions in a game where we start with 11 gold pieces and buy two items. first, we buy a sword at 5 gold pieces and then buy a shield at five gold pieces, and both transactions are directed to different nodes of our distributed database. each node reads the value, which in this case is still 11 for both nodes. both nodes will decide to write 6 as the result (11- 5) since they are not aware of any replication. since the second transaction could not see the value of the first write yet, the player ends up buying both the sword and shield for five gold pieces total instead of 10. good for the user, but not so good for the system! to remedy such behavior, distributed databases have several strategies — some better than others. impact of lost writes on users. in this case, the user succeeds in buying two items while paying only once. resolution strategies include “last write wins" (lww) or "longest version history" (lvh) wins. lww has for a long time been the strategy of cassandra and is still the default behavior if you do not configure it differently. if we apply lww conflict resolution to our previous example, the player will still be left with 6 gold, but will only have bought one item. this is a bad user experience because the application confirmed his purchase of the second item, even though the database doesn't recognize it as existing in his inventory. an example of simple conflict resolution. two transactions on different nodes are changing the amount of gold at the same time. the writes initially go through but when the two nodes communicate, the conflict becomes apparent. the conflict resolution strategy here is to cancel one of the transactions. the user can no longer try to take advantage of the system but occasionally writes will be lost. unpredictable security as you might imagine, it is unsafe to write security rules on top of such a system. many applications rely on complex security rules in the backend (or directly on the database where possible) to determine whether a user can or cannot access a resource. when these rules are based on stale data that's updated unreliably, how can we be sure that there is never a breach? imagine one user of a paas application calls his administrator and asks: “could you make this public group private so that we can repurpose it for internal data?” the admin applies the action and tells him it’s done. however, because the admin and user might be on different nodes, the user might start adding sensitive data to a group that is technically still public. impact on developers when writes are lost, debugging user issues will be a nightmare. imagine that a user reports that he has lost data in your application, then one day goes by before you get time to respond. how will you try to find out whether the issue was caused by your database or by faulty application logic? in a database that allows tracking data history such as faunadb or datomic, you would be able to travel back in time to see how the data had been manipulated. neither of these is vulnerable to lost writes though, and databases that do suffer from this anomaly typically don’t have the time-travel feature. databases that suffer from lost writes all databases that use conflict resolution instead of conflict avoidance will lose writes. cassandra and dynamodb use last write wins (lww) as default; mongodb used to use lww but has since moved away from it. the master-master distribution approaches in traditional databases such as mysql offer different conflict resolution strategies. many distributed databases that were not built for consistency suffer from lost writes. riak’s simplest conflict resolution is driven by lww, but they also implement more intelligent systems. but even with intelligent systems, sometimes there's just no obvious way to resolve a conflict. riak and couchdb place the responsibility to choose the correct write with the client or application, allowing them to manually choose which version to keep. since distribution is complex and most databases use imperfect algorithms, lost writes are common in many databases when nodes crash or when network partitions arise. even mongodb, which does not distribute writes (writes go to one node), can have write conflicts in the rare case that a node goes down immediately after a write. 3. write skew write skew is something that can happen in a type of guarantee that database vendors call snapshot consistency. in snapshot consistency, the transaction reads from a snapshot that was taken at the time the transaction started. snapshot consistency prevents many anomalies. in fact, many thought it was completely secure until papers (pdf) started to appear proving the opposite. therefore, it’s not a surprise that developers struggle to understand why certain guarantees are just not good enough. before we discuss what doesn't work in snapshot consistency, let's first discuss what does. imagine that we have a battle between a knight and a mage, whose respective life powers consist of four hearts. when either character gets attacked, the transaction is a function that calculates how many hearts have been removed: damagecharacter(character, damage) {
  character.hearts = character.hearts - damage
  character.dead = ischaracterdead(character)
} and, after each attack, another ischaracterdead function also runs to see if the character has any hearts left: ischaracterdead(character) {
  if ( character.hearts <= 0 ) { return true }
  else { return false }
} in a trivial situation, the knight's strike removes three hearts from the mage, and then the mage's spell removes four hearts from the knight, bringing his own life points back to four. these two transactions would behave correctly in most databases if one transaction runs after the other. but what if we add a third transaction, an attack from the knight, which runs concurrently with the mage's spell? example of two transactions (life leech and the second powerful strike) that will determine the outcome of the battle. what would be the outcome in a system that provides snapshot consistency? to know that we have to learn about the ‘first committer wins’ rule. is the knight dead, and is the mage alive? to deal with this confusion, snapshot consistency systems typically implement a rule called “the first committer wins.” a transaction can only conclude if another transaction did not already write to the same row, else it will roll back. in this example, since both transactions tried to write to the same row (the mage’s health), only the life leech spell would work and the second strike from the knight would be rolled back. the end result would then be the same as in the previous example: a dead knight and a mage with full hearts. however, some databases such as mysql and innodb do not consider “the first committer wins” as part of a snapshot isolation. in such cases, we would have a lost write: the mage is now dead, although he should have received the health from the life leech before the strike of the knight took effect. (we did mention badly defined terminology and loose interpretations, right?) snapshot consistency that includes the “first committer wins” rule does handle some things well, not surprising since it was considered a good solution for a long time. this is still the approach of postgresql, oracle, and sql server, but they all have different names for it. postgresql calls this guarantee "repeatable read," oracle calls it "serializable" (which is incorrect according to our definition), and sql server calls it "snapshot isolation." no wonder people get lost in this forest of terminology. let’s look at examples where it is not behaving as you would expect! impact on end users the next fight will be between two armies, and an army is considered dead if all of the army characters are dead: isarmydead(army){
  if (<all characters are dead>) { return true }
  else { return false }
} after every attack, the following function determines if a character has died, and then runs the above function to see if the army has died: damagearmycharacter(army, character, damage){
  character.hearts = character.hearts - damage
  character.dead = ischaracterdead(character)
  armydead = isarmydead(army)
  if (army.dead !=  armydead){
    army.dead = armydead
  }
} first, the character’s hearts are diminished with the damage that was received. then, we verify whether the army is dead by checking whether each character is out of hearts. then, if the state of the army has changed, we update the ‘dead’ boolean of army. example of write skew, an anomaly that can happen in databases that provide snapshot consistency. there are three mages that each attack one time resulting in three ‘life leech’ transactions. snapshots are taken at the beginning of the transactions, since all transactions start at the same time, the snapshots are identical. each transaction has a copy of the data where all knights still have full health. let’s take a look at how the first ‘life leech’ transaction resolves. in this transaction, mage1 attacks knight1, and the knight loses 4 life points while the attacking mage regains full health. the transaction decides that the army of knights is not dead since it can only see a snapshot where two knights still have full health and one knight is dead. the other two transactions act on another mage and knight but proceed in a similar way. each of those transactions initially had three live knights in their copy of the data and only saw one knight dying. therefore, each transaction decides that the army of knights is still alive. when all transactions are finished, none of the knights are still alive, yet our boolean that indicates whether the army is dead is still set to false. why? because at the time the snapshots were taken, none of the knights were dead. so each transaction saw his own knight dying, but had no idea about the other knights in the army. although this is an anomaly in our system (which is called write skew), the writes went through since they each wrote to a different character and the write to the army never changed. cool, we now have a ghost army! impact on developers data quality what if we want to make sure users have unique names? our transaction to create a user will check whether a name exists; if it does not, we will write a new user with that name. however, if two users try to sign up with the same name, the snapshot won’t notice anything since the users are written to different rows and therefore do not conflict. we now have two users with the same name in our system. there are numerous other examples of anomalies that can occur due to write skew. if you are interested, martin kleppman’s book “designing data-intensive applications” describes more. code differently to avoid the rollbacks now, let’s consider a different approach where an attack is not directed towards a specific character in the army. in this case, the database is responsible for selecting which knight should be attacked first. damagearmy(army, damage){
  character = getfirsthealthycharacter(knight)
  character.hearts = character.hearts - damage
  character.dead = ischaracterdead(character)
  // ...
} if we execute several attacks in parallel as in our previous example, the getfirsthealthycharacter will always target the same knight, which would result in multiple transactions that write to the same row. this would be blocked by the “first committer wins” rule, which will roll back the two other attacks. although it prevents an anomaly, the developer is required to understand these issues and code around them creatively. but wouldn’t it be easier if the database just did this for you out-of-the-box? databases that suffer from write skew any database that provides snapshot isolation instead of serializability can suffer from write skew. for an overview of databases and their isolation levels, please refer to this article. 4. out of order writes to avoid lost writes and stale reads, distributed databases aim for something called "strong consistency." we mentioned that databases can either choose to agree on a global order (the safe choice) or decide to resolve conflicts (the choice that leads to lost writes). if we decide on a global order, it would mean that although the sword and shield are bought in parallel, the end result should behave as if we bought the sword first and then bought the shield. this is also often called "linearizability" since you can linearize the database manipulations. linearizability is the gold standard to make sure your data is safe. different vendors offer different isolation levels, which you can compare here. a term that comes back often is serializability which is a slightly less strict version of strong consistency (or linearizability). serializability is already quite strong and covers most anomalies, but still leaves room for one very subtle anomaly due to writes that get reordered. in that case, the database is free to switch that order even after the transaction has been committed. linearizability in simple terms is serializability plus a guaranteed order. when the database is missing this guaranteed order, your application is vulnerable to out of order writes. impact on end users reordering of conversations conversations can be ordered in a confusing way if someone sends a second message due to a mistake. reordering of user actions if our player has 11 coins and simply buys items in the order of importance while not actively checking the amount of gold coins he has, then the database can reorder these buy orders. if he didn't have enough money, he could have bought the item of least importance first. in this case, there was a database check which verified whether we have enough gold. imagine that we did not have enough money and it would cost us money to let the account go below zero, just like a bank charges you overdraft fees when you go below zero. you might sell an item quickly in order to make sure you have enough money to buy all three items. however, the sale that was meant to increase your balance might be reordered to the end of the transaction list, which would effectively push your balance below zero. if it were a bank, you would likely incur charges you definitely did not deserve. unpredictable security when an invulnerability spell swaps order with an axe attack after configuring security settings, a user will expect that these settings will apply to all forthcoming actions, but issues can arise when users talk to each other via different channels. remember the example we discussed where an administrator is on the phone with a user who wants to make a group private and then adds sensitive data to it. although the time window within which this can happen becomes smaller in databases that offer serializability, this situation can still occur since the administrator's action might not be completed until after the user’s action. when users communicate through different channels and expect that the database is ordered in real-time, things go wrong. this anomaly can also happen if a user is redirected to different nodes due to load balancing. in that case, two consecutive manipulations end up on different nodes and might be reordered. if a girl adds her parents to a facebook group with limited viewing rights, and then posts her spring break photos, the images might still end up in her parents’ feeds. in another example, an automatic trading bot might have settings such as a maximum buy price, a spending limit, and a list of stocks to focus on. if a user changes the list of stocks that the bot should buy, and then the spending limit, he will not be happy if these transactions were reordered and the trading bot has spent the newly allocated budget on the old stocks. impact on developers exploits some exploits depend on the potential reversal of transactions. imagine that a game player receives a trophy as soon as he owns 1,000 gold, and he really wants that trophy. the game calculates how much money a player has by adding together gold of multiple containers, for example his storage and what he's carrying (his inventory). if the player quickly swaps money in between his storage and inventory, he can actually cheat the system. in the illustration below, a second player acts as a partner in crime to make sure that the money transfer between the storage and the inventory happens in different transactions, increasing the chance that these transactions get routed to different nodes. a more serious real world example of this happens with banks that use a third account to transfer money; the bank might miscalculate whether or not someone is eligible for a loan because various transactions have been sent to different nodes and not had enough time to sort themselves out. databases that suffer from out of order writes any database that does not provide linearizability can suffer from write skew. for an overview of which databases do provide linearizability, please refer to this article. spoiler: there are not that many. all anomalies can return when consistency is bounded one final relaxation of strong consistency to discuss is to only guarantee it within certain bounds. typical bounds are a datacenter region, a partition, a node, a collection, or a row. if you program on top of a database that imposes these kinds of boundaries to strong consistency, then you need to keep those in mind to avoid accidentally opening pandora's box again. below is an example of consistency, but only guaranteed within one collection. the example below contains three collections: one for the players, one for the smithies (i.e., blacksmiths repairing players' items), and another for the items. each player and each smithy has a list of ids that point to items in the items collection. if you want to trade the shield between two players (e.g., from brecht to robert), then everything is fine since you remain in one collection and therefore your transaction remains within the boundaries where consistency is guaranteed. however, what if robert's sword is in the smithy for repairs and he wants to retrieve it? the transaction then spans two collections, the smithy's collection and the player's collection, and the guarantees are forfeited. such limitations are often found in document databases such as mongodb. you will then be required to change the way you program to find creative solutions around the limitations. for example, you could encode the location of the item on the item itself. of course, real games are complex. you might want to be able to drop items on the floor or place them in a market so that an item can be owned by a player but does not have to be in the player's inventory. when things become more complex, these workarounds will significantly increase technical depth and change the way you code to stay within the guarantees of the database. consistency with limitations often requires you to be aware of the limitations and change the way you code, stepping out of the boundary, and again exposing your application to the aforementioned anomalies. conclusion we have seen different examples of issues that can arise when your database does not behave as you would expect. although some cases might seem insignificant at first, they all have a significant impact on developer productivity, especially as a system scales. more importantly, they open you up to unpredictable security exploits — which can cause irreparable damage to your application's reputation. we discussed a few degrees of consistency, but let’s put them together now that we have seen these examples: stale reads lost writes write skew out of order writes linearizability safe safe safe safe serializability safe safe safe unsafe snapshot consistency safe safe unsafe unsafe eventual consistency unsafe unsafe unsafe unsafe also remember that each of these correctness guarantees can come with boundaries: row-level boundaries the guarantees delivered by the database are only honored when the transaction reads/writes to one row. manipulations such as moving items from one player to another can cause issues. hbase is an example database that limits guarantees to one row. collection-level boundaries the guarantees delivered by the database are only honored when the transaction reads/writes to one collection. e.g., trading items between two players stays within a "players" collection, but trading them between a player and an entity from another collection such as a market opens the door to anomalies again. firebase is an example which limits correctness guarantees to collections. shard/replica/partition/session boundaries as long as a transaction only affect data on one machine or shard, the guarantees hold. this is, of course, less practical in distributed databases. cassandra has recently started offering serializability features if you configure them, but only within a partition. regionboundaries some databases almost go all the way and provide guarantees across multiple nodes (shards/replicas), but their guarantees do not hold anymore if your database is distributed across multiple regions. such an example is cosmos. cosmos is a great technology, but they have chosen an approach where consistency guarantees are limited to one region. finally, realize that we have only mentioned a few anomalies and consistency guarantees while in fact there are more. for the interested reader, i fondly recommend martin kleppman’s designing data-intensive applications. we live in a time when we no longer have to care, as long as we choose a strongly consistent database without limitations. thanks to new approaches such as calvin (faunadb) and spanner (google spanner, foundationdb), we now have multi-region distributed databases that deliver great latencies and behave as you expect in each scenario. so why would you still risk shooting yourself in the foot and choose a database that does not deliver these guarantees? in the next article in this series, we will go through the effects on your developer experience. why is it so hard to convince developers that consistency matters? spoiler: most people need to experience it before they see the necessity. think about this though: “if bugs appear, is your app wrong, or is it the data? how can you know?” once the limitations of your database manifest themselves as bugs or bad user experiences, you need to work around the limitations of the database, which results in inefficient glue code that does not scale. of course, at that point, you are deeply invested and the realization came too late. article series why should you care? what can go wrong? what are the barriers to adoption? how do new algorithms help? our learning partner need some front-end development training? frontend masters is the best place to get it. they have courses on all the most important front-end technologies, from react to css, from vue to d3, and beyond with node.js and full stack. need some front-end development training? frontend masters is the best place to get it. they have courses on all the most important front-end technologies, from react to css, from vue to d3, and beyond with node.js and full stack. leave a reply cancel reply your email address will not be published. required fields are marked * comment name * email * website save my name, email, and website in this browser for the next time i comment. get the css-tricks newsletter notify me of follow-up comments by email. copy and paste this code: micuno * leave this field empty all comments are held for moderation. we'll publish all comments that are on topic, not rude, and adhere to our code of conduct. you'll even get little stars if you do an extra good job. you may write comments in markdown. this is the best way to post any code, inline like `<div>this</div>` or multiline blocks within triple backtick fences (```) with double new lines before and after. want to tell us something privately, like pointing out a typo or stuff like that? contact us. we have a code of conduct. be cool. be helpful. the web is a big place. have fun. high five. related how do you stay up to date in this fast⁠-⁠moving industry? a good start is to sign up for our weekly hand-written newsletter. we bring you the best articles and ideas from around the web, and what we think about them. email address subscribe facebook @csstricks twitter @css youtube @realcsstricks instagram @real_css_tricks rss feed css-tricks* is created, written by, and maintained by chris coyier and a team of swell people. the tech stack for this site is fairly boring. that's a good thing! i've used wordpress since day one all the way up to v17, a decision i'm very happy with. i also leverage jetpack for extra functionality and local for local development. *may or may not contain any actual "css" or "tricks". other nav contact about guest writing jobs advertise license subscribe all authors codepen codepen is a place to experiment, debug, and show off your html, css, and javascript creations. flywheel logo flywheel css-tricks is hosted by flywheel, the best wordpress hosting in the business, with a local development tool to match. shoptalk is a podcast all about front-end web design and development. css-tricks presents 📅 upcoming front-end conferences css-tricks presents the power of serverless for front-end developers