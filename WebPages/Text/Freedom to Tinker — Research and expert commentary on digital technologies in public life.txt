march 27, 2020 posts comments freedom to tinker research and expert commentary on digital technologies in public life vulnerability reporting is dysfunctional march 25, 2020 by kevin lee 2 comments by kevin lee, ben kaiser, jonathan mayer, and arvind narayanan in january, we released a study showing the ease of sim swaps at five u.s. prepaid carriers. these attacks—in which an adversary tricks telecoms into moving the victim’s phone number to a new sim card under the attacker’s control—divert calls and sms text messages away from the victim. this allows attackers to receive private information such as sms-based authentication codes, which are often used in multi-factor login and password recovery procedures. we also uncovered 17 websites that use sms-based multi-factor authentication (mfa) and sms-based password recovery simultaneously, leaving accounts open to takeover from a sim swap alone; an attacker can simply reset a victim’s account password and answer the security challenge when logging in. we responsibly disclosed the vulnerabilities to those websites in early january, urging them to make changes to disallow this configuration. throughout the process, we encountered two wider issues: (1) lack of security reporting mechanisms, and (2) a general misunderstanding of authentication policies. as a result, 9 of these 17 websites, listed below, remain vulnerable by default. disclosure process. on each website, we first looked for email addresses dedicated to vulnerability reporting; if none existed, we looked for the companies on bug bounty platforms such as hackerone. if we were unable to reach a company through a dedicated security email or through bug bounty programs, as a last resort, we reached out through customer support channels. sixty days after our reports, we re-tested the configurations at the companies, except for those that reported that they had fixed the vulnerabilities. outcomes. three companies—adobe, snapchat, and ebay—acknowledged and promptly fixed the vulnerabilities we reported. in one additional case, the vulnerability was fixed, but only after we exhausted the three contact options and reached out to company personnel via a direct message on twitter. in three cases—blizzard, microsoft, and taxact—our vulnerability report did not produce the intended effect (microsoft and taxact did not understand the issue, blizzard provided a generic acknowledgment email), but in our 60-day re-test, we found that the vulnerabilities had been fixed (without the companies notifying us). as such, we do not know whether the fixes were implemented in light of our research. among the responses we received, there were several failure modes, which were not mutually exclusive. in five cases, personnel did not understand our vulnerability report, despite our attempts to make it as clear as possible (see appendix b of our paper). three of them—microsoft, paypal, and yahoo—demonstrated knowledge of sim swap attacks, but did not realize that their sms authentication policies were allowing for vulnerable accounts. paypal, for instance, closed our report as out-of-scope, claiming that “the vulnerability is not in paypal, as you mentioned this is an issue with the carriers and they need to fix it on their side.” while phone number hijackings are the result of poor customer authentication procedures at the carriers, account hijackings resulting from sms passcode interception are the result of poor authentication policies at websites. the remaining two websites—taxact and gaijin entertainment—misinterpreted our disclosure as a feature request and feedback, respectively. three of the four reports we submitted to third-party bug bounty programs were disregarded due to the absence of a bug (our findings are not software errors, but rather, logically inconsistent customer authentication policies). reports are screened by employees of the program, who are independent of the website, and passed on to the website’s security teams if determined to be in scope. these third-party platforms appear to be overly strict with their triage criteria, preventing qualified researchers from communicating with the companies. this issue is not unique to our study, either. a few weeks ago, security researchers also reported difficulties with submitting vulnerability reports to paypal, which uses hackerone as its sole security reporting mechanism. hackerone employs mechanisms that restrict users from submitting future reports after too many closed reports, which could disincentivize users from reporting legitimate vulnerabilities. in five cases, we received no response. all four attempts to report security vulnerabilities through customer support channels were fruitless: either we received no response or personnel did not understand the issue. we have listed all 17 responses in the table below. unfortunately, nine of these websites use sms-based mfa and sms-based password recovery by default and remain so as of this writing. among them are payment services paypal and venmo. the vulnerable websites cumulatively have billions of users. recommendations we recommend that companies make the following changes to their vulnerability response: companies need to realize that policy-related vulnerabilities are very real, and should use threat modeling to detect these. there seems to be a general lack of knowledge about vulnerabilities arising from weak authentication policies. companies should provide direct contact methods for security reporting procedures. a bug bounty program is not a substitute for a robust security reporting mechanism, yet some companies are using it as such. furthermore, customer support channels—whose personnel are unlikely to be trained to respond to security vulnerability disclosures—add a level of indirection and can lead to vulnerability reports being forwarded to inappropriate teams. our paper, along with our dataset, is located at issms2fasecure.com. thanks to malte möser for providing comments on a draft. filed under: privacy & security tagged with: authentication, responsible disclosure, vulnerability disclosure building a bridge with concrete… examples march 23, 2020 by angelina wang leave a comment thanks to annette zimmermann and arvind narayanan for their helpful feedback on this post. algorithmic bias is currently generating a lot of lively public and scholarly debate, especially amongst computer scientists and philosophers. but do these two groups really speak the same language—and if not, how can they start to do so? i noticed at least two different ways of thinking about algorithmic bias during a recent research workshop on the ethics of algorithmic decision-making at princeton university’s center for human values, organized by political philosopher dr. annette zimmermann. philosophers are thinking about algorithmic bias in terms of things like the inherent value of explanation, the fairness and accountability rights afforded to humans, and whether groups that have been systematically affected by unfair systems should bear the burden for integration when transitioning to a uniform system. computer scientists, by contrast, are thinking about algorithmic bias in terms of things like running a gradient backwards to visualize a heat map, projecting features into various subspaces devoid of protected attributes, and tuning hyperparameters to better satisfy a new loss function. of course these are vast generalizations about the two fields, and there are plenty of researchers doing excellent work at the intersection, but it seems that for the most part while philosophers are debating which sets of ethical axioms ought to underpin algorithmic decision-making system, computer scientists are in the meantime already deploying these systems into the real world. in formulating loss functions, consequentialists might prioritize maximizing accurate outcomes for the largest possible number of people, even if that is at the cost of fair treatment, whereas deontologists might prioritize treating everyone fairly, even if that is at the cost of optimality. but there isn’t a definitive “most moral” answer, and if something like equalizing false positive rates were the key to fairness, we would not be having the alarming headlines of algorithmic bias that we have today. inundated with various conflicting definitions of fairness, scientists are often optimizing for metrics they believe to be best and proceeding onwards. for example, one might reasonably think that the way to ensure fairness of an algorithm between different racial groups could be to enforce predictive parity (equal likelihood of accurate positive predictions), or to equalize false error rates, or just to treat similar individuals similarly. however, it is actually mathematically impossible to simultaneously satisfy seemingly reasonable fairness criteria like these in most real world settings. it is unclear how to choose amongst the criteria, and even more unclear how one would go about translating complex ideas that may require consideration, such as systematic oppression, into a world of optimizers and gradients. since concrete mappings between a mathematical loss function and moral concepts are likely impossible to dictate, and philosophers are unlikely to settle on an ultimate theory of fairness, perhaps for now we can adopt a strategy that is, at least, not impossible to implement: a purposefully created, context- and application-specific validation/test set. the motivation behind this is that even if philosophers and ethicists cannot decisively articulate a set of general, static fairness desiderata, perhaps they can make more domain-specific, dynamic judgements: for instance whether one should prefer a system that gives person a with a set of attributes and features a loan or not. and they can also say that for person b and c and so on. of course there will not be unanimous agreement, but at least a general consensus towards a particular outcome as preferable over the other. one could then create a whole set of such examples. concepts like the idea that similar people should be treated similarly in a given decision scenario—the ‘like cases maxim’ in legal philosophy—could be encoded into this test set by having groups of people that differ only in a protected attribute be given the same result, and even concepts like equal accuracy rates across protected groups could be encoded in by having the test set be represented by equal numbers of people from each group rather than proportional to the real world majority/minority representations. however, the test set is not a constructually valid way to enforce these fairness constraints, and it shouldn’t be either, because the reason why such a test set would exist is that the right fairness criteria are not actually known, otherwise it would just be explicitly formulated into the loss function. at this juncture, ethicists and computer scientists could usefully engage in complementary work: ethicists could identify difficult edge cases that challenge what we think about moral questions and incorporate this into the test set, and computer scientists could work on optimizing accuracy rates on a given validation set. there are a few crucial differences, however, from similar collaborative approaches in other domains like when doctors are called on to provide expert labels on medical data so models can be trained to detect things like eye diseases. there is now the new notion that the distribution of the test set, in addition to just the labels, are going to be specifically decided upon by domain experts. further, this collaboration would last beyond just the labeling of the data. failure cases should be critically investigated earlier in the machine learning pipeline in an iterative and reflective way to ensure things like overfitting are not happening. whether performing well on the hidden test set requires learning fairer representations in the feature space or thresholding different groups differently, scientists will build context-specific models that encompass certain moral values defined by ethicists, who are grounding the test set in examples of realizations of such values. but does this proposal mean adopting a potentially dangerous, ethically objectionable “the ends justify the means” logic? not necessarily. with algorithm developers working in conjunction with ethicists to ensure the means are not unsavory, this could be a way to bridge the divide between abstract notions of fairness, and concrete ways of implementing systems. this may not be a long-term ideal way to deal with the problem of algorithmic fairness because of the difficulty in generalizing between applications, and in situations where creating an expert-curated test set is too expensive or not scalable, not preferred over satisfying one of the many mathematical definitions of fairness, but it could be one possible way to incorporate philosophical notions of fairness into the development of algorithms. because technologists are not going to hold off and wait on deploying machine learning systems until they are in a state of fairness everyone agrees on, finding a way of incorporating philosophical views about central moral values like fairness and justice into algorithmic systems right now is an urgent problem. supervised machine learning has traditionally been focused on predicting based on historical and existing data, but maybe we can structure our data in a way that is a model not of the society we actually live in, but of the one we hope to live in. translating complex philosophical values into representative examples is not an easy task, but it is one that ethicists have been doing a version of for centuries in order to investigate moral concepts—and perhaps it can also be the way to convey some sense of our morals to machines. filed under: uncategorized the cheapbit of fitness trackers apps march 16, 2020 by yan shvartzshnaider leave a comment yan shvartzshnaider (@ynotez) and madelyn sanfilippo (@mrsmrs_phd) fitness trackers are “[devices] that you can wear that records your daily physical activity, as well as other information about your health, such as your heart rate” [oxford dictionary]. the increasing popularity of wearable devices offered by apple, google, nike inadvertently led cheaper versions to flood the market, along with the emergence of alternative non-tech, but fashionable brand devices. cheaper versions ostensibly offer similar functionality for one-tenth of the price, which makes them very appealing to consumers. on amazon, many of these devices receive overall positive feedback and an average of 4-5 star reviews. some of them are even labeled as “amazon’s choice” and “best buyer” (e.g. figure 1), which reinforces their popularity. in this blog post, we examine privacy issues around these cheaper alternatives devices, specifically focusing on the ambiguities around third party apps they are using. we report our preliminary results into a few apps that seem to dominate the marketspace. note that fashion brands also employ third party apps like wearos by google, but they tend to be more recognizable and subject to greater consumer protection scrutiny. this makes them different than lesser-known devices. figure 1: letscom, uses veryfitpro, with over 13k reviews, labeled as amazon’s choice and is marketed to children. do consumers in fact pay dearly for the cheaper version of these devices? privacy issues are not unique to cheaper brands. any “smart device” that has the ability to collect, process and share information about you and the surrounding environment, can potentially violate your privacy. security issues also play an important role. services like mozilla’s privacy not included and consumer reports help navigate the treacherous landscape. however, even upholding the minimum security standards doesn’t prevent privacy violations due to inappropriate use of information, see strava and polar incidents. given that most of the analysis is typically done by an app paired with a fitness tracker, we decided to examine the “cheapbit” products sold on amazon, with a large average number of reviews and answered questions, to see which apps they pair with. we found that the less-expensive brands are dominated by a few third-party apps primarily developed by small teams (or individuals) and do not provide any real description as to how data are used and shared. but what do we know about these apps? the veryfitpro app seems to be the choice of many of the users buying the cheaper fitness trackers alternatives. the app has 5,000,000+ installs, according to google play, where it lists an email of the developer *protected email* and the website with just a qr code to download the app. the app has access to an extensive list of permissions: sms, camera, location, wifi information, device id & call information, device & app history, identity, phone, storage, contacts, and photo/media/files! the brief privacy policy appears to be translated into english using an automatic translation tool, such as google translate. surprisingly, what appears to be the same app on the apple store points to a different privacy policy altogether, hosted on a facebook page! the app provides a different contact email ( *protected email*) and policy is even shorter than on the play store. in a three-paragraph policy, we are reassured that “some of your fitness information and sports data will be stored in the app, but your daily activities data will never be shared without permission.” and with a traditional “we reserve the right, in our decision to change, modify, add or remove portions of this policy at any time. please check this page periodically for any changes. publish any changes to these terms if you continue to use our app future will mean that you have accepted these adjustments. [sic]” no additional information is provided. while we found the veryfitpro to be common among cheap fitness trackers, especially high-rated ones, it is not unique. other apps such as jyoupro, which has access to the same range of permissions, offer privacy policy which is just two paragraphs long which also reassures users that “[they] don’t store personal information on our servers unless required for the on-going operation of one of our services.” the apple version offers a slightly longer version of the policy. in it, we find that “when you synchronise the band data, e.g. to jyoupro cloud service, we may collect data relating to your activities and functionalities of jyoupro, such as those obtained from our sensors and features on jyoupro, your sleeping patterns, movement data, heart rate data, and smart alarm related information.” given that jyoupro is used by a large number of devices, their “cloud service” seems to be sitting on a very lucrative data set. the policy warns us: “please note also that for the above, jyoupro may use overseas facilities operated and controlled by jyoupro to process or back up your personal data. currently, jyoupro has data centres in beijing and singapore.” these are however not the worst offenders. developers behind apps like morepro and wearfit didn’t even bother to translate their privacy policies from chinese! users’ privacy concerns these third-party apps are incredibly popular and pervade the low-end wearable market: veryfitpro ( 5,000,000+ installs), jyoupro (500,000+ installs), wearfit (1,000,000+ installs). with little oversight, they are able to collect and process lots of potentially sensitive information from having access to contacts, camera, location, and other sensors data from a large number of users. most of them are developed by small teams or unknown chinese firms, which dominate the mhealth market. a small portion of users on amazon express privacy concerns. for one of the top selling products letscom fitness tracker which uses veryfitpro with 4/5 stars, 14,420 ratings and 1000+ answered questions, marketed towards “kids women and men”, we were able to find only a few questions on privacy. notably, none of the questions was upvoted, so we suspect the remain unseen by the typical buyer. for example, one user was asking “what is the privacy policy for the app? how secure is the personal information? [sic]” to which another user (not the manufacturer) replied “a: this connects to your phone by bluetooth. that being said, i guess you could connect it only when you are in a secure location but then you wouldn’t have the message or phone notifications.” a similar concern was raised by another user “what is this company’s policy on data privacy? will they share or sell the data to third parties?” in another popular product, lintelek fitness tracker with heart rate monitor which used veryfitpro with 4/5 stars, 4,050 ratings. out of 1000+ answered questions, only a couple mentioned privacy. the first user gave a product 1 start with ominous warning “be sure to read the privacy agreement before accepting this download”. interestingly, the second user rated the product with 5 stars and gave a very positive review that ends with “only con: read the privacy statement if you are going to use the text/call feature. they can use your information. i never turned it on – i always have my phone anyway.” the fact that buyers of these devices do not investigate the privacy issues is troubling. previous research showed that consumers will think that if a company has a privacy policy it protects their privacy. it seems to be clear that consumers need help from the platform. amazon, google and apple ought to better inform consumers about potential privacy violations. in addition to consumer protection obligations by these platforms, regulators ought to apply increased scrutiny. while software are not conventional medical devices, hence not covered by hipaa, some medical apps do fall under fda authority, including apps that correspond with wearables. furthermore, as in figure 1 shows, these devices are marketed to children so the app should be subject to enforcement of children’s privacy standards like coppa. in conclusion, the lesser-known fitness tracking brands offer a cheaper alternative to high-end market products. however, as previous research showed, consumers of these devices are potentially paying a high-privacy price. the consumers are left to fend for themselves. in many cases, the cheaper devices pertaining to firms outside of us jurisdiction and thus us and european regulations are difficult to enforce. furthermore, global platforms like amazon, google, apple, and others seem to turn a blind eye to privacy issues and help to promote these devices and apps. they offer unhelpful and possibly misleading labels to the consumers such as amazon’s “best seller”, “amazon’s choice”, google’s play store’s download count and star ratings, which exacerbate an already global and complex issue. it requires proactive action on behalf of all parties to offer lasting protection of users’ privacy, one that incorporates the notions of established societal norms and expectations. we would like to thank helen nissenbaum for offering her thoughts on the topic. filed under: internet of things, privacy & security next page » freedom to tinker is hosted by princeton's center for information technology policy, a research center that studies digital technologies in public life. here you'll find comment and analysis from the digital frontier, written by the center's faculty, students, and friends. what we discuss aacs bitcoin cd copy protection censorship citp competition copyright cross-border issues cybersecurity policy dmca drm education events facebook fcc government government transparency grokster case humor innovation policy law managing the internet media misleading terms nsa online communities patents peer-to-peer predictions princeton privacy publishing recommended reading secrecy security spam super-dmca surveillance tech/law/policy blogs technology and freedom transparency virtual worlds voting wiretapping wpm contributors select author... aleecia m. mcdonald alex halderman and nadia heninger alex migicovsky andrew appel angelina wang annemarie bridy annette zimmermann annie edmundson arunesh mathur arvind narayanan axel arnbak aylin caliskan-islam bart huffman barton gellman bendert zevenbergen bill zeller blake reid brett frischmann bryan ford dan wallach daniel howe dave levine david lukens david robinson diego vicentin dillon reisman ed felten elena lucherini eric smith and nina kollars ethan heilman gary mcgraw gina neff grayson barber gunes acar harlan yu harry kalodner hooman mohajeri moghaddam ian davey ian lundberg j. alex halderman james grimmelmann jared ho jasmine peled joanna bryson jd lasica jeffrey tignor jennifer rexford jeremy epstein jerry brito jessica su joe calandrino joel reidenberg jon penney jonathan mayer joseph bonneau joseph lorenzo hall joshua goldstein joshua kroll julia stoyanovich karen eltis katherine haenschen kelvin chen kevin lee laura cummings-abdo leonid reyzin liza paudel luis villa lukasz olejnik madelyn r sanfilippo marcela melara mark hass marshini chetty matheus ferreira matthew salganik mihir kshirsagar mike freedman miles carlsten mitch golden nadia heninger nathan matias nick feamster nicky robinson paul ellenbogen paul ohm paulina borsook pete zimmerman philip n. howard philipp winter pmittal priya kumar rebecca mackinnon ron hedges ronaldo lemos sam ransbotham sg stephen schultze steve roosa steven englehardt steven goldfeder suman jana tiffany li timothy b. lee vanessa teague and j. alex halderman vitaly shmatikov wendy seltzer will clarkson yan shvartzshnaider yoshi kohno yusuf dahl zeynep tufekci archives by month 2020: j f m a m j j a s o n d 2019: j f m a m j j a s o n d 2018: j f m a m j j a s o n d 2017: j f m a m j j a s o n d 2016: j f m a m j j a s o n d 2015: j f m a m j j a s o n d 2014: j f m a m j j a s o n d 2013: j f m a m j j a s o n d 2012: j f m a m j j a s o n d 2011: j f m a m j j a s o n d 2010: j f m a m j j a s o n d 2009: j f m a m j j a s o n d 2008: j f m a m j j a s o n d 2007: j f m a m j j a s o n d 2006: j f m a m j j a s o n d 2005: j f m a m j j a s o n d 2004: j f m a m j j a s o n d 2003: j f m a m j j a s o n d 2002: j f m a m j j a s o n d author log in return to top of page copyright © 2020 ·education theme on genesis framework · wordpress · log in