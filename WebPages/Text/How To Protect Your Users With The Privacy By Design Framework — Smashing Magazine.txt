skip to main content start reading the article jump to list of all articles search topics articles . design & development books . physical & digital books events . conferences & workshops jobs . find work & employees membership . webinars & early-birds browse all topics clear search browse all topics accessibility android animation apps css design design patterns design systems e-commerce freebies graphics html illustrator inspiration ios javascript mobile pattern libraries performance photoshop plugins react responsive web design service workers sketch typography ui usability user experience wallpapers web design wordpress workflow about the author heather burns is aech policy and regulation specialist from glasgow, scotland. she researches, writes, and speaks extensively on laws, regulations, and … more about heather burns … july 27, 2017 leave a comment how to protect your users with the privacy by design framework 15 min read ethics, apps, privacy share on twitter or linkedin smashing newsletter upgrade your inbox and get our editors’ picks 2× a month — delivered right into your inbox. earlier issues. your (smashing) email subscribe → in these politically uncertain times, developers can help to defend their users’ personal privacy by adopting the privacy by design (pbd) framework. these common-sense steps will become a requirement under the eu’s imminent data protection overhaul, but the benefits of the framework go far beyond legal compliance. note: this article is not legal advice and should not be construed as such. meet privacy by design let’s give credit where credit is due. the global political upheaval of the past 12 months has done more to get developers thinking about privacy, surveillance and defensive user protection than ever before. the risks and threats to ourselves, and to our users, are no longer theoretical; they are real, they are everyday, and they are frightening. one need only look at the ongoing revelations regarding cambridge analytica, a british company with odd links to canada, which ran a complex data-mining operation on behalf of donald trump’s presidential campaign to aggregate up to 5,000 pieces of data on every american adult, to fathom what is at stake for all of us. as developers and decision-makers, we need to do something to respond to that challenge. the political uncertainty we are living through obliges us to change the ways we approach our work. as the creators of applications and the data flows they create, we can play a critical and positive role in protecting our users from attacks on their privacy, their dignity, and even their safety. one way we can do this is by adopting a privacy-first best-practice framework. this framework, known as privacy by design (pbd), is about anticipating, managing and preventing privacy issues before a single line of code is written. the best way to mitigate privacy risks, according to the pbd philosophy, is not to create them in the first place. pbd has existed as a best-practice framework since the 1990s, but few developers are aware of it, let alone use it. that’s about to change. the eu’s data protection overhaul, gdpr, which becomes legally enforceable in may 2018, requires privacy by design as well as data protection by default across all uses and applications. as with the previous eu data protection regime, any developer serving european customers must adhere to these data protection standards even if they themselves are not located in europe. so, if you do business in or sell to europe, privacy by design is now your responsibility. this presents a monumental opportunity for developers everywhere to rethink their approach to privacy. let’s learn what pbd is and how it works. see also: privacy guidelines for designing personalization driving app engagement with personalization techniques real-time data and a more personalized web what is pbd? the pbd framework was first drawn up in canada in the 1990s. its originator, dr. ann cavoukian, then privacy commissioner of ontario, devised the framework to address the common issue of developers applying privacy fixes after a project is completed: the privacy by design framework prevents privacy-invasive events before they happen. privacy by design does not wait for privacy risks to materialize, nor does it offer remedies for resolving privacy infractions once they have occurred; it aims to prevent them from occurring. in short, privacy by design comes before-the-fact, not after. the pbd framework has seven foundational principles: privacy must be proactive, not reactive, and must anticipate privacy issues before they reach the user. privacy must also be preventative, not remedial. privacy must be the default setting. the user should not have to take actions to secure their privacy, and consent for data sharing should not be assumed. privacy must be embedded into design. it must be a core function of the product or service, not an add-on. privacy must be positive sum and should avoid dichotomies. for example, pbd sees an achievable balance between privacy and security, not a zero-sum game of privacy or security. privacy must offer end-to-end lifecycle protection of user data. this means engaging in proper data minimization, retention and deletion processes. privacy standards must be visible, transparent, open, documented and independently verifiable. your processes, in other words, must stand up to external scrutiny. privacy must be user-centric. this means giving users granular privacy options, maximized privacy defaults, detailed privacy information notices, user-friendly options and clear notification of changes. why pbd matters more than ever pbd has always been available for any developer to use as a voluntary best-practice framework. its popularity has tended to be greater in cultures that have a traditionally positive view of privacy, such as canada and many european countries. privacy, however, is not traditionally seen as a positive value in the us, whose companies dominate the tech world. for this reason, and for too long, web development has been approached with what at times has felt like the complete opposite of a pbd viewpoint. it has almost become normal for developers to ship apps that require social media registration, that request unnecessary permissions such as microphone access and location data, and that demand access to all of a user’s contacts. the notion of privacy by design as a voluntary concept is about to change. in europe, the regulation that governs all collection and processing of personal data, regardless of use, sector or situation, has had a complete overhaul. this new set of rules, known as the general data protection regulation (gdpr), is already on the books but becomes legally enforceable on 25 may 2018. your business should already be working towards your wider gdpr compliance obligations ahead of this deadline, which will come up fast. crucially, gdpr makes pbd and privacy by default legal requirements within the eu. not only will you have to develop to pbd, but you will have to document your pbd development processes. that documentation must be made available to a european regulatory authority in the event of a data breach or a consumer complaint. “what if i am not in the eu?” remember that european data protection and privacy laws are extraterritorial: they apply to the people within europe whom data is collected about, regardless of where the service is provided from. in other words, if you develop for european customers, you must comply with eu data protection and privacy standards for those individuals, even if you yourself are not located within europe. also remember that the eu, through its data protection system, has some of the strictest and most clearly defined privacy frameworks in the world; the us, by contrast, has no overarching data protection and privacy framework at all. culture is important to remember as well. in europe, privacy is considered a fundamental human right. living and developing in a country where privacy is not a fundamental human right does not negate your moral or legal obligations to those who do enjoy that right. developers outside the eu, therefore, should consider adopting the pbd principles within the gdpr guidelines as a development framework, despite being located outside europe. the guidelines will give you a clear, common-sense and accountable framework to use in your development process — and that framework is a lot better than having no guidelines at all. what is personal data? the european data protection law defines personal data as any information about a living individual who could be identified from that data, either on its own or when combined with other information. there is also a classification called “_sensitive personal data_”, which means any information concerning an individual’s racial or ethnic origin political opinions, religious or philosophical beliefs, trade union membership, health data, genetic data, biometric data, sex life or sexual orientation, past or spent criminal convictions. the data users generate within your app is personal data. their account information with your company is personal data. the uid identifying their device is personal data. so is their ip address, their location data, their browser fingerprint and any identifiable telemetry. practical pbd implementation for app developers, pbd compliance means factoring in data privacy by default at your app’s initial design stage, throughout its lifecycle, throughout the user’s engagement with your app, after the user’s engagement has ended, and after the app is mothballed. there is no checklist of ready-made questions that will get you there; general data protection regulation requires developers to come up with the questions as well as the answers. but in a proactive development environment, the answers would likely take the practical forms required under gdpr, such as the following. design stage create a privacy-impact assessment template for your business to use for all functions involving personal data, which we will come to a bit later on. review contracts with partners and third parties to ensure the data you pass on to them is being processed in accordance with pbd and gdpr. don’t require unnecessary app permissions, especially those that imply privacy invasion, such as access to contacts or to the microphone. audit the security of your systems, which we will also come to shortly. lifecycle minimize the amount of collected data. minimize the amount of data shared with third parties. where possible, pseudonymize personal data. revisit contact forms, sign-up pages and customer-service entry points. enable the regular deletion of data created through these processes. user engagement provide clear privacy- and data-sharing notices. embed granular opt-ins throughout those notices. don’t require social media registration to access the app. don’t enable social media sharing by default. separate consent for essential third-party data sharing from consent for analytics and advertising. end of engagement and mothballing periodically remind users to review and refresh their privacy settings. allow users to download and delete old data. delete the data of users who have closed their accounts. delete all user data when the app’s life comes to an end. pbd in action good pbd practice, and its absence, is easy to spot if you know what you are looking for. let’s give this popular uk pub chain’s app a quick pbd audit. the app has no settings page, which suggests no user control over privacy. this implies that downloading the app entails granting consent for data-sharing, which does not meet the second pbd principle. this suggestion is confirmed in the “edit account” option, which only allows users to edit their name and email address. this does not meet the third pbd principle “privacy must be embedded into design.” the link to the privacy policy provides a blurry scan of a five-page pdf that is written for the outdated 1995 data protection directive. there are no user controls or granular options. if i wanted to exercise control over my data, i would have to write an email to a generic customer-service address or, alternatively — in the time-honored tradition of privacy policies that are really trying to fob users off — send them a letter in the post. this does not meet the seventh pbd principle of giving users granular privacy options with maximized privacy defaults. the privacy policy informs me that my data will be shared with third parties but gives me no indication as to who those third parties are, nor does it give me the option to withhold that data sharing. there is no distinction between third parties whose services are necessary for the transaction (for example, paypal) and unnecessary services, such as ad networks. this does not meet the sixth pbd principle. but let’s say i write about this stuff for a living, and so i just really need a beer. i go to the pub and fire up the wi-fi to use its app. when i connect to the wi-fi, i notice its “settings” page. that page merely provides links to three legal documents. as with the pub’s own app, there are no settings to change, no options and no choices. there is no pbd whatsoever. it’s clear that the only way i can ensure my privacy with this pub chain is to not use the app or its wi-fi at all. this creates the zero-sum dichotomy, which the fourth pbd principle seeks to avoid. this pub chain does not meet good pbd practice, or gdpr compliance, by any definition. by contrast, twitter’s recent privacy overhaul demonstrates very good pbd practice and early gdpr compliance. here’s the catch: its new privacy choices could have a detrimental and negative effect on users’ privacy. the difference, however, is that it has been open and transparent and has given users educated choices and options. the privacy overhaul offers a range of granular privacy options, which are clearly communicated, including a clear notice that it may share your data with third parties. users can disable some or all options. a prominent splash screen drew users’ attention to the changes, increasing the likelihood that they would take the time to educate themselves on their privacy options. twitter is clearly using a pbd framework well in advance of gdpr. privacy impact assessment a privacy impact assessment (pia) is simply a process of documenting the issues, questions and actions required to implement a healthy pbd process in a project, service or product. pias are a core requirement of gdpr, and in the event of a data protection issue, your pia will determine the shape of your engagement with a regulatory authority. you should use a pia when starting a new project, and run a pia evaluation of any existing ones. the steps in a pia are as follows: identify the need for a pia. describe the information flows within a project or service (user to service provider, user to user, service provider to user, user to third parties, service provider to third parties). identify the privacy- and data-protection risks. identify and evaluate the privacy solutions. sign off and record the pia outcomes. integrate the outcomes into the project plan. consult with internal and external stakeholders as needed throughout the process. take some time to come up with a pia template unique to your business or project that you can use as needed. the guidance from ico, the uk data-protection regulator, will help you to do that. privacy information notice good pbd practice gives users clear information and informed choices. privacy information notices are central to that. the days of privacy policies being pages upon pages of dense legal babble, focused on the needs of the service provider and not the user, are over. your app, product or service should have a privacy information notice, including the following details: what data are you collecting? why are you collecting it, and is that reasoning legally justifiable? which third parties are you sharing it with? what third-party data are you aggregating it with? where are you getting that information from? how long are you keeping it? how can the user invoke their rights? include any information regarding the use of personal data to fulfil a contract. many european data protection regulators are devising standardized templates for privacy information notices, and you should check with yours to follow the progress on any required format ahead of the may 2018 deadline. under gdpr, the old privacy policy trick of stating “we may share your data with third parties” will no longer be considered compliant. gdpr and pbd require you to list exactly who those parties are and what they do with user data. as one dramatic example, paypal’s recent updated notice lists over 600 third-party service providers. the fact that paypal shares data with up to 600 third parties is not news. that information is simply being brought into the open. security measures good pbd compliance is not just about ux. healthy compliance also involves implementing adequate technical and security measures to protect user data. these measures, as with other aspects of full gdpr compliance, must be documented and made accountable to a regulator on request. pbd compliance on a technical and security level could include: password hashing and salting; data sandboxing; pseudonymization and anonymization; automated background updates; encryption at rest and in transit; responsible disclosure; staff training and accountability on data protection; physical security of servers, systems and storage. data protection officer under gdpr, companies processing certain kinds of data must appoint a data protection officer (dpo), a named individual with legally accountable responsibility for an organization’s privacy compliance, including pbd. this requirement is regardless of a company’s size, which means that even the tiniest business engaged in certain kinds of data processing must appoint a dpo. a dpo does not have to be in-house or full-time, nor are legal qualifications required. very small businesses can appoint a dpo on an ad-hoc or outsourced basis. check with your eu member state’s data-protection regulator for information on your national requirements for a dpo. we would encourage all organizations to voluntarily appoint a dpo regardless of the nature of their work. think of a dpo as the health and safety officer for privacy. having someone act as the “good cop” to keep your development processes legally compliant — and acting as the “bad cop” if your practices are slipping — can save you from a world of troubles down the road. change your thinking the pbd framework poses challenges that only you can answer. no one else can do it for you: it is your responsibility to commence the process. if you are within europe, have a look at your national data-protection regulator’s gdpr and pbd resources. if you are outside europe, we have provided some links and resources below. don’t view pbd as a checklist of boxes to be ticked because “the law says so,” nor think of it as something you have to do “or else.” instead, use pbd to think really creatively. think of all the ways that your users’ data can be misused, accessed, stolen, shared or combined. think of where data might be located, even if you might not be aware of it. think of what liabilities you might be creating for yourself by collecting, retaining and aggregating data that you don’t really need. think of how the third parties you share data with, even if they are your business partners, could create liabilities for you. and in our current political climate, think about the ways that the data you collect and process could be used to do harm to your users. there are no wrong questions to ask, but there are questions that it would be wrong not to ask. adopting pbd into your development workflow will create new steps to follow and new obligations to meet. these steps, as onerous as they might feel, are necessary in our rapidly changing world. so, view pbd as a culture shift. use it as an opportunity to improve your policies, practices and products by incorporating privacy into your development culture. your users will be better protected, your business’s reputation will improve, and you will be well on the road to healthy legal compliance. in an often bewildering world, if these steps are all we can take to make a difference one app at a time, they are worth a lot. more information and resources “privacy by design,” information and privacy commissioner of ontario an introduction. conducting privacy impact assessments: code of practice” (pdf), information commissioner’s office privacy in mobile apps: guidance for app developers” (pdf), information commissioner’s office “data protection: better rules for small business,” european commission overview of the eu’s gdpr. “privacy design guidelines for mobile application development,” gsma (al) browse all smashing magazine topics accessibility android animation apps css design design patterns design systems e-commerce freebies graphics html illustrator inspiration ios javascript mobile pattern libraries performance photoshop plugins react responsive web design service workers sketch typography ui usability user experience wallpapers web design wordpress workflow with a commitment to quality content for the design community. founded by vitaly friedman and sven lennartz. 2006–2020. smashing is proudly running on netlify. fonts by latinotype. ✎ write for us contact us about us (impressum) privacy policy membership login delivery times advertise back to top