daverupert.com home archive about maintaining performance or... how i shaved ~33s off my page load by fixing fonts march 20, 2020 a while back i was able to shave off ~33 seconds from my page load time by fixing how i load fonts. shaved off ~33s from my page load this morning by... fixing how i load fonts? pic.twitter.com/jxcoaf1as7 ‚Äî dave rupert (@davatron5000) november 5, 2019 surprising, i know! in my previous attempts to follow cutting-edge best practices i made an honest mistake: <link rel="preload" href="/fonts/rubik.woff2"/>
<link rel="preload" href="/fonts/rubik-bold.woff2"/>
<link rel="preload" href="/fonts/domine.woff2"/>
<link rel="stylesheet" href="/css/fonts.css" media="print" onload="this.media='all'">
 looks fast, right? i‚Äôm preloading my font files so they show up fast as possible and then do a little asynchronous switcheroo to prevent my @font-face rule stylesheet from blocking. well‚Ä¶ i fell into a trap. preloading three fonts meant other things wouldn‚Äôt load as quickly and this became a bottleneck that i didn‚Äôt notice due to service workers or my fast connection. so i burned it all down and went back to plain ol‚Äô css font-face with font-display: swap. @font-face {
	font-family: "rubik";
	font-display: swap;
  src: url("/fonts/rubik.woff2") format("woff2"),
       url("/fonts/rubik.woff") format("woff");
}

...
 the font-display property is an amazing new tool that didn‚Äôt exist the last time i worked on my site. font-display lets me specify how and when i‚Äôd like my fonts to be applied. the swap value means my fonts will ‚Äúswap‚Äù in whenever they‚Äôre ready without blocking the page load, which is what i was trying to do with that lazy css trick. now that my fonts are lazily applied, i didn‚Äôt even need the separate stylesheet anymore. it‚Äôs a bit counterintuitive, but backing away from preloading tricks resulted in my site becoming much faster, with fewer requests, and became easier to maintain. it‚Äôs totally possible layering in preload again would produce a tangible benefit, but i learned a lesson to take a more iterative approach with better monitoring. this is all to say‚Ä¶ this story is less about webfont performance and is actually framing for another point i‚Äôm trying to make. i, dave rupert, a person who cares about web performance, a person who reads web performance blogs, a person who spends lots of hours trying to keep up on best practices, a person who co-hosts a weekly podcast about making websites and speak with web performance professionals‚Ä¶ somehow goofed and added 33 seconds to their page load. i find that web performance isn‚Äôt particularly difficult once you understand the levers you can pull; reduce kilobytes, reduce requests, unblock rendering, defer scripts. but maintaining performance that‚Äôs a different story entirely‚Ä¶ over time on any project i‚Äôve ever worked on, uncontrollable factors start to creep in; ttfb slow down from your web host, marketing goes on a tracking script spree in google tag manager, those show third parties have slow third-parties, cpus have major vulnerabilities, clearing technical debt gets pushed to the backlog, and browsers make minor tweaks to loading algorithms. ‚Äúbest practices‚Äù, i tend to feel, change every 6~12 months: ‚Äúuse preload‚Äù obviously didn‚Äôt scale for me. ‚Äúuse blurry images‚Äù could probably be ‚Äúuse loading="lazy" with height and width‚Äù now, but some people feel that loading="lazy" is too eager! ü§∑ ‚Äúdo {x,y,z} for fonts‚Äù is a never-ending enterprise. ‚Äúbundle your javascript‚Äù is currently morphing into ‚Äúonly bundle for ie11‚Äù‚Ä¶ oh, and you should be writing all your non-ui code in workers now‚Ä¶ the list goes on. it‚Äôs also worth noting web performance fixes usually fall under the umbrella of architecture. updating a best practice tends to be a deep fix and involves updating a build process, a global template, or low-level/highly-depended-on component. doing and undoing those types of performance changes takes effort. in my experience, 99% of the time web performance boils down to two problems: ‚Äúyou wrote too much javascript.‚Äù ‚Äúyou have unoptimized images.‚Äù which, okay. fine. the javascript is pretty hard to dig out of and most companies simply won‚Äôt. that‚Äôs unfortunate for their users and the self-justification from developers is just white noise to me at this point. the images issue, however, is a fairly ubiquitous one and there‚Äôs three ways out of that jam: pay some service forever. concoct some ginormous build process. constant vigilance and manual labor to ensure best quality per kilobyte. and doesn‚Äôt that just sum up nearly every problem in web development? use a third-party, build some crufty thing yourself, or spend massive amounts of energy establishing a weak process-based culture. sometimes i wonder what life would be like if i always chose option #1. automating away some problems i like doing web performance manually, so that i have leathered hands that understand the problem. but given that web performance is a shifting problem and a garden you must tend to, perhaps automating away what i can is a smart choice. i have a short todo list of automated processes i‚Äôd like to try and setup and evaluate before passing these recommendations onto a client. perhaps i can carve some time out in these dreary times to experiment and play, but that‚Äôs asking a lot. i‚Äôm looking at adding lighthouse ci as a github action. this way, i could solve the ‚Äúconstant vigilance‚Äù problem and move the needle from manual monitoring to proactively be notified of any performance regressions. it would also be cool to use lighthouse as a netlify build plugin since i typically use netlify as a ci, but i don‚Äôt think it collects historical data at this point. another route would be using something like speedcurve or calibre to introduce monitoring. while these services are great, they‚Äôre priced more for larger (more competitive) businesses than my personal site. $20~$60 per month for my blog would be a 2.5x~5x increase in operation costs. i‚Äôm also looking at using thumbor as an image cdn to solve the unoptimized image problem. i‚Äôve long dreamed of being able to self-host my own media server that does all the lossless optimization and adaptive serving (like webp when supported) for me. this adds a ~$5/month on digital ocean maintenance cost just to slim down images. i also just learned that netlify large media has image transformation but requires moving over to gitlfs and doesn‚Äôt appear to do adaptive serving. i feel if i could get these two automations ironed out, i‚Äôd have an easier time maintaining performance at a minimal cost. if you‚Äôve had success on this, i‚Äôd love to know more about your setup. please enable javascript to view the comments powered by disqus. home ‚Ä¢ about ‚Ä¢ archive ‚Ä¢ likes ‚Ä¢ bookshelf ‚Ä¢ rss ¬© 2020 dave rupert ‚Ä¢ twitter ‚Ä¢ github