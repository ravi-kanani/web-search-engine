skip to main content start reading the article jump to list of all articles search topics articles . design & development books . physical & digital books events . conferences & workshops jobs . find work & employees membership . webinars & early-birds browse all topics clear search browse all topics accessibility android animation apps css design design patterns design systems e-commerce freebies graphics html illustrator inspiration ios javascript mobile pattern libraries performance photoshop plugins react responsive web design service workers sketch typography ui usability user experience wallpapers web design wordpress workflow about the author jeremy wagner is a performance-obsessed front end developer, author and speaker living and working in the frozen wastes of saint paul, minnesota. he is also the … more about jeremy wagner … october 5, 2016 leave a comment next generation server compression with brotli 18 min read coding, servers, performance, node.js, https share on twitter or linkedin smashing newsletter upgrade your inbox and get our editors’ picks 2× a month — delivered right into your inbox. earlier issues. your (smashing) email subscribe → chances are pretty good that you’ve worked with, or at least understand the concept of, server compression. by compressing website assets on the server prior to transferring them to the browser, we’ve been able to achieve substantial performance gains. for quite some time, the venerable gzip algorithm has been the go-to solution for reducing the size of page assets. a new kid on the block has been gaining support in modern browsers, and its name is brotli. in this article, you’ll get hands-on with brotli by writing a node.js-powered http server that implements this new algorithm, and we’ll compare its performance to gzip. chances are pretty good that you’ve worked with, or at least understand the concept of, server compression. by compressing website assets on the server prior to transferring them to the browser, we’ve been able to achieve substantial performance gains. for quite some time, the venerable gzip algorithm has been the go-to solution for reducing the size of page assets. a new kid on the block has been gaining support in modern browsers, and its name is brotli. in this article, you’ll get hands-on with brotli by writing a node.js-powered http server that implements this new algorithm, and we’ll compare its performance to gzip. further reading on smashingmag: front-end performance checklist 2017 improving smashing magazine’s performance: a case study meet imageoptim-cli, a batch compression tool efficient image resizing with imagemagick introducing brotli brotli is a compression algorithm maintained by google and first released in 2015. its namesake is a swiss pastry product. it was not initially released for use as a standalone algorithm (as gzip was), but rather as an offline compression solution for the woff2 font format. this means that if you’ve been using woff2 fonts, you’ve already been using brotli and you didn’t even know it! later in 2015, brotli went beyond providing offline compression of woff2 fonts. brotli is now supported in a large segment of browsers as a new accept-encoding scheme that we can use to compress page assets like we’ve been doing with gzip, but with a reported improvement in compression ratios. this is an enticing prospect for the performance-minded developer. hold up! what’s the browser support? browser support for brotli is not universal. the following browsers support brotli out of the box, without requiring you to turn on support for them under the hood: chrome since version 50, android browser version 50, chrome for android since version 50, firefox since version 44, firefox for android since version 46, opera since version 38. while this list implies that edge, safari and others have left brotli support out in the cold (for now, at least), caniuse.com indicates that its support is estimated at around 53% at the time of writing. of course, this statistic will fluctuate over time, so see for yourself what the support status is for this technology. either way, we’re not talking about a small segment of users who would potentially benefit from the increased performance that this new algorithm provides, so it’s worth investigating to see what the gains are. before jumping in with both feet, however, we should talk about the requirement that browsers have for this feature — namely, https. we meet again, https it’s hardly news that browser vendors have been advocating for the transition to a more secure web, and for good reason: https is no longer the burden it once was in terms of cost and performance. in fact, thanks to modern hardware and the http/2 protocol’s multiplexing of requests over a single connection, the overhead of https connections is less than you might think. in terms of cost, ssl certificates are downright cheap, at as little as $5 a year, depending on the reputation of the signing authority. if that cost is still a barrier for you, you can rely on let’s encrypt for free ssl certificates. the barrier to entry for regular folks who need a secure website couldn’t be much flimsier than it is today, and that’s how it should be. as an additional motivator, browser vendors have been making ssl a de facto requirement for all sorts of new features, such as service workers, http/2 and, yes, even brotli. we can see this requirement in action by visiting any secure website and examining any asset’s accept-encoding request header for the br token in a brotli-enabled browser: the br token in the accept-encoding request header, as seen in google chrome if you go to a non-secure website over http and look at the value of the same request header for any asset, you’ll see that the br token is absent. i’m sure by now you’ve had enough of the hype and are ready to get your hands dirty with brotli. so, let’s get started by writing a small web server in node.js using the express framework, and implement brotli using the shrink-ray package. building a brotli-enabled web server in node.js adding brotli to existing web servers such as nginx or apache can prove to be inconvenient, depending on your familiarity with them. a brotli module does exist for nginx, as does one for apache, but building and running the apache module requires some know-how. that’s fine if you’re cool with that sort of thing, but most of us just want to install something and get right to the tinkering! so, to make things a little easier on ourselves, i’ll be showing you how to set up a small brotli-capable server written in javascript using node.js and express. even if you’ve never used these technologies, don’t worry. all you’ll need before you begin is to have a copy of node.js installed; you’ll be guided through the entire process. before you know it, you’ll have a brotli-powered web server up and running on your local machine, ready for your scrutiny. installing the prerequisites because our testing server is in https, we’ll need to have a certificate and key handy. generating these can be a chore. to make things easier, you can clone the certificates and directory structure we need by using git:    
git clone https://github.com/malchata/brotli-server.git
    
 this will download a github repository with our certificate and key files in the crt directory, and an empty web root directory of htdocs. you can enter into the repository directory by typing cd brotli-server. (want to skip ahead? if you’re not terribly interested in writing the web server code from scratch and want to get right to messing with brotli, you can skip ahead by switching to a branch with the completed code by typing git checkout -f brotli-server.) in order for the server to work, we’ll need to install a few packages using npm:    
npm install express https shrink-ray
    
 this will install three packages: express is the express framework package. this is used to spin up a simple static web server that will serve content from the htdocs directory. https is the package that enables us to serve files over https. shrink-ray is the compression middleware that contains the brotli functionality we want to test. it includes gzip functionality as well. note: if you’re doing all of this on windows, this package relies on node-gyp, which can be problematic for windows users. you’ll have better luck if you have a linux subsystem, such as the one available on windows 10. chances are that if you’re developing for node on windows, you’re aware of the idiosyncrasies. if not, read this comment in a github gist regarding the subject. installing these dependencies might take a minute. once it’s finished, you’ll be ready to write your web server code! writing the web server code in your text editor of choice, create a new javascript file named https.js, and begin with the following code:    
var express = require(“express”), // imports the express package
    https = require(“https”), // imports the https package
    shrinkray = require(“shrink-ray”), // imports the compression middleware
    fs = require(“fs”), // the file system module for reading files (part of node.js core)
    path = require(“path”), // the path module for working with files and directory paths (also part of node.js core)
    app = express(), // an express instance
    pubdir = “./htdocs”; // the web root directory
    
 in case you’re somewhat new to node.js, the require method imports the modules we need for use in the current script. the pubdir variable is what we’ll use to refer to the htdocs directory, which is where we’ll serve files from. continuing on, we’ll need to set up our compression middleware from the shrink-ray package by telling our express instance in the app object to use it. we’ll also instruct our express instance to statically serve files from the htdocs directory:    
app.use(shrinkray()); // tell express to use the shrink-ray compression middleware
app.use(express.static(path.join(__dirname, pubdir))); // tell express to serve static files from the htdocs directory
    
 we’ll top it all off by setting up our https server and running it on port 8443:    
https.createserver({ // creates an instance of an https sever
    key: fs.readfilesync(“crt/localhost.key”), // reads in the key file
    cert: fs.readfilesync(“crt/localhost.crt”) // reads in the ssl certificate
}, app).listen(8443); // passes in our express instance and instructs the server to run on port 8443
    
 now for the moment of truth, when we run our brotli-powered web server:    
node https.js
    
 if all has gone well, no errors should occur and the server will start. to test it out, point your browser to https://localhost:8443/readme.txt, and you should see a short message. if you’ve gotten to this point, you’re ready to verify that brotli is working. how can you tell that brotli is working? by default, shrink-ray will compress content with brotli if the requesting browser supports it and if the server is running on https. the easiest way to check for support is to grab a copy of a javascript library (such as react) and save it in the htdocs directory. from here, open chrome or firefox and launch the developer tools. you can do this by pressing f12 on a windows machine or command + alt + i on a mac. once the tools are open, click on the “network” tab. the “network” tab is a common utility available in chrome and firefox’s developer tools that shows all of the network requests for a given web page. with this tab open, navigate to the asset you saved in the htdocs folder on the local web server. you’ll see the network utility populate with the requested resource. in chrome, we can see the value of an asset’s content-encoding header in the network utility’s “content-encoding” column. if this column is not visible, just right-click on the column headers and choose it from the menu that appears. if brotli is working, you should see a br token in the “content-encoding” column similar to what’s shown in the image below: the br token in the accept-encoding response header, as seen in chrome's network utility (view large version) now that we’ve verified that brotli is running on our local test web server, let’s see how brotli performs compared to gzip! evaluating brotli’s performance now to the meat of the matter: how well does brotli perform compared to gzip? if you don’t want to do a ton of testing, there’s a performance test that gives a good overview of brotli’s performance. the test is set up to download assets from popular websites specified in a text file, and once the assets are gathered, the test process commences, as specified in the github repository’s readme document: read the file’s contents into memory. take a timestamp to mark the start of the test. compress the file 100 times using brotli level 1. take a timestamp to mark the end of the test. record the compressed file size and compression speed (in mb per second). repeat steps 2 to 5 for brotli levels 2 to 11. repeat steps 2 to 5 for gzip level 6. output the results in json format. the number of websites specified in the benchmark’s text file is huge, and thus the test takes a very long time to complete. in the interest of time, i specified 20 websites that i frequent (including this website) and ran the benchmark over them instead. i felt that this would still provide a good high-level view of brotli’s performance on all compression levels compared to the default gzip setting of 6. the average compression speeds of all brotli settings versus the default gzip setting are shown in the table below: algorithm compression level speed (mb per second) gzip 6 11.8 brotli 1 41.5 brotli 2 16 brotli 3 13.6 brotli 4 6.83 brotli 5 5.98 brotli 6 5.8 brotli 7 0.966 brotli 8 0.758 brotli 9 0.555 brotli 10 0.119 brotli 11 0.121 as said, this is a very high-level overview. the test collects a lot of data, but average compression speed gives us a basic idea of how brotli compares to gzip’s default compression level. the one flaw of this test is that it doesn’t collect data for all gzip settings from 1 to 9. it also can’t really quantify how brotli influences website loading times, because the test measures compression performance on files already on the disk. despite this, this overview is somewhat indicative of what you’ll see in the following tests, in that higher compression settings are going to be the slowest. we just need to see how this influences website loading times and how it compares to all available gzip settings. to fill in the blanks a bit, i’ve done some of my own performance testing. first, we’ll look at how well all brotli compression settings compare to all gzip settings for a single asset. then, we’ll do the same for a node.js-driven website running on a local machine that is bandwidth-throttled using chrome’s network-throttling utility. then, we’ll do the same again, but for an apache-driven website using the mod_brotli compression module. testing methods when testing, i wanted to pick a javascript library that is popular and also very large. react fits the bill perfectly, coming in at 144 kilobytes minified. this seems like a reasonable test subject for comparing compression algorithm performance for a single file. when comparing compression algorithms, we also want to know more than what the final size of a compressed asset is. while this dimension is strongly tied to page-loading time, it’s important to note that it’s not a consistent relationship in every scenario. compressing content eats up cpu time, and if an algorithm is too cpu-intensive, there’s a chance that any gains made in compression ratios will be nullified if the algorithm takes too long to do its job. therefore, we want to know two dimensions: the final file size and the amount of time it takes for the compressed asset to load. however, simply comparing gzip and brotli out of the box isn’t enough. we can adjust the settings for both of these technologies, and when we do so, we influence their performance. gzip allows us to specify a compression level between 0 and 9, with 0 turning off compression altogether. brotli can similarly be set between 1 and 11. gzip’s default is 6, and the default that the shrink-ray package sets for brotli is 4. we can set brotli’s compression level like so:    
app.use(shrinkray({
    brotli: {
        quality: 11 // compression level configurable from 1 to 11
    }
}));
    
 in the table below is a comprehensive collection of final file sizes when compressing the selected javascript library on all configurable levels for both brotli and gzip. numbers are in kilobytes, and the lowest file sizes are underlined and bolded. level gzip (kb) brotli (kb) 1 50.4 48.6 2 48.6 44.8 3 47.4 44.1 4 44.5 42.9 5 43.2 40.2 6 42.8 39.8 7 42.7 39.5 8 42.6 39.4 9 42.6 39.3 10 n/a 36.8 11 n/a 36.2 at a glance, we can see that the gains are quite impressive. at the highest compression level, brotli outdoes gzip by 6.4 kilobytes, which is no small amount of data. as stated earlier, though, trade-offs can occur when compression levels are sufficiently high. let’s see how loading times are affected across the various compression levels: level gzip (milliseconds) brotli (milliseconds) 1 640.6 623.8 2 626 577.8 3 610.2 578.2 4 578 563.2 5 568 534.8 6 564.6 532 7 569.2 514.4 8 567.4 514 9 563 517.2 10 n/a 558.8 11 n/a 704.6 because the test server runs locally, i ran the test in chrome using the “regular 3g” profile in the network-throttling utility, to simulate what loading times would be like over a slow mobile connection. each figure is the average of five test runs. in instances where direct comparisons can be made, brotli seems to perform better in both file-size yield and loading time. once we hit compression levels 10 and 11, however, we started to see hugely diminishing returns. even though these compression levels yield much smaller file sizes, the computational overhead erases the gains made in file size. the shrink-ray package makes up for this overhead in its own way with a caching mechanism. in these tests, that caching mechanism was disabled to get an accurate picture of brotli’s performance with on-the-fly compression. the default behavior of shrink-ray is to first compress the response at the default quality setting. while that happens, the same asset is asynchronously compressed at the highest-quality setting and then cached for subsequent requests. this caching mechanism yields a loading time of around 480 milliseconds for the react library. note that this caching feature does not come standard with brotli, but is rather how shrink-ray is designed to work. any module that implements brotli may or may not cache entries for recently compressed assets. performance in a real scenario all of this seems rather clinical, because we’re not actually applying this to a real website, but rather to a single file. to get an idea of real-world performance, i took a client’s website and ran it through the wringer on my local computer. i tested loading times over the varying quality levels for brotli with caching disabled, and then with compression caching enabled to see how the shrink-ray package performs when left to its own devices. below are comparisons of loading times using the same methodology outlined earlier: level gzip (milliseconds) brotli (milliseconds) 1 871.4 869.2 2 869.2 848.4 3 868 858.4 4 845 850.2 5 850.8 857.8 6 852.8 844.8 7 867.8 846.4 8 860.4 833.8 9 847.8 832.6 10 n/a 825.2 11 n/a 849 11 (cached) n/a 823.2 in this case, we’re able to take a website that would otherwise be 52.4 kb at the highest gzip setting of 9, and reduce its payload to 48.4 kb with brotli’s highest setting of 11. this is a reduction of about 8%, and after the caching takes effect, we can reduce loading times further. bear in mind that this example is of a small website. your mileage may vary. that’s not to say that there won’t be a benefit for websites with larger payloads, only that you should perform your own analysis before fully implementing brotli for your website. another scenario we can look at is a wordpress blog that runs on an apache server. legendary tones is a website that i host for a friend. although the mod_brotli module for apache is in its nascent stage, it works well enough that we can test with it. i pulled down the website and ran it on my local apache server, and i tested all available settings for both mod_deflate and mod_brotli. the conditions for this test are the same as before: throttle the bandwidth using chrome’s throttling utility at the “regular 3g” setting, but instead of 5 trials, i performed 20. level gzip (milliseconds) brotli (milliseconds) 1 3060 3064 2 2968 2980 3 3004 2914 4 2900 2894 5 2910 2772 6 2858 2758 7 2836 2806 8 2854 2896 9 2998 2990 10 n/a 2910 11 n/a 2766 in most scenarios where direct comparisons can be made, brotli seems to outperform gzip, even if only by a little bit. however, let’s examine a few caveats for all of the tests we’ve done: these tests were done on a local web server whose only traffic was me. while brotli yields significantly lower file sizes at the highest compression levels, the loading times of these assets usually tend to suffer at the 10 and 11 quality settings. if we can cache the compressed response ahead of time, we can negate the long processing time of higher brotli compression levels. shrink-ray does this for us automatically, but other implementations might lack this caching mechanism. if you’re willing to test brotli for your projects, you’ll get a better idea of whether it’s a good fit. the good news is that if you set up your web server properly, browsers that don’t support brotli will just fall back to gzip, meaning that everyone will get some benefit, regardless of what algorithms are supported. for example, here’s a line from my website’s apache configuration that implements both mod_brotli and mod_deflate:    
addoutputfilterbytype brotli;deflate text/html text/css application/javascript text/javascript image/svg+xml text/plain text/xml application/x-javascript
    
 the key piece of this configuration directive is the brotli;deflate portion. when both the mod_brotli and mod_deflate modules are loaded, we can specify what compression algorithm is preferred. by placing brotli first in the chain, browsers that support it will receive content compressed by it. in the event that a browser comes along that doesn’t support brotli, it will be served by gzip (deflate) instead. with our time together coming to an end, let’s take a minute to cover a bit of what we’ve learned about brotli. conclusion my findings at this time tell me that you have every good reason to do some research and see what’s possible with brotli on your website. in most situations, it seems that brotli can squeeze a bit more performance out of your websites, which might be worth pursuing. while brotli really starts to get sluggish at higher compression levels, striking a good balance can provide some level of benefit. i can’t possibly give any sweeping generalizations as to what compression settings are good for all websites. you’ll just need to test on your own. i highly recommend using this approach to see what the results are for you and to see what implementations exist for your server. if you’re serving pages with node.js, nginx or apache, you’ve got options. furthermore, it’s worth noting that brotli is a continually evolving project. google’s github repository of the project shows that contributions are made regularly, and that’s reason enough for the performance-minded web developer to keep an eye on this promising new technology. this article is about a relatively new alternative to gzip compression, named brotli. this and many other topics are covered in jeremy’s book web performance in action, which you can get from manning publications for 38% off with the coupon code smashmagpc, as well as any other manning book! (rb, al, il) browse all smashing magazine topics accessibility android animation apps css design design patterns design systems e-commerce freebies graphics html illustrator inspiration ios javascript mobile pattern libraries performance photoshop plugins react responsive web design service workers sketch typography ui usability user experience wallpapers web design wordpress workflow with a commitment to quality content for the design community. founded by vitaly friedman and sven lennartz. 2006–2020. smashing is proudly running on netlify. fonts by latinotype. ✎ write for us contact us about us (impressum) privacy policy membership login delivery times advertise back to top